{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP (Natural Language Processing)\n",
    "* Use cases: Auto complete, Translation, NER (Name Entity Recognition), Sentiment Analysis, Music composition\n",
    "* Helps solve sequence models better than ANNs by using: Variable size of input/output neurons\n",
    "* Text Preprocessing: Tokenization, Stop Words, Lemmatization, Bag of Words, TFIDF, Word Embeddings\n",
    "* Model building: Bidirectional LSTM RNN, GRU, Encoders and Decoders, Attention Models\n",
    "* Transformers, BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing using NLTK\n",
    "\n",
    "* Tokenization\n",
    "    * Assigns each word an index.\n",
    "    * Limitations: Presence of Dot: Ph.D --> Ph, D , Presence of Blank Spaces: San Francisco --> San, Francisco\n",
    "    \n",
    "* Stop Words\n",
    "    * Removing words that don't add any context to the sentence. Makes model computationally efficient\n",
    "\n",
    "* Stemming\n",
    "    * Strips a word from tense, prefix, suffix and stores the stem word. PorterStemmer, LancasterStemmer\n",
    "    \n",
    "* Lemmatization\n",
    "    * Stemming is a best guess where to cut, Lemmatization is more calculated and computationally heavy\n",
    "    * Tenses are resolved\n",
    "    * Context is important: Lemmatization, Speed is important: Stemming\n",
    "    \n",
    "* Word Embeddings\n",
    "    * Methods to convert words to numbers: \n",
    "        * Use unique numbers, One hot encoding: Context not considered, Computationally inefficient \n",
    "        * Word Embeddings: Context considered, Computationally efficient: Bag of Words, TF-IDF, Word2Vec, Embedding Layer\n",
    "        * Word Embeddings gives us x_train and x_test for machine learning model building\n",
    "        * x_bow, x_tfidf can be split into x_train and x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "# # To update nltk libraries\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Fishers are not very known for fishing in the #canal. \n",
    "Something fishy was going on, some of the fish dove into the lake\"\"\"\n",
    "\n",
    "# # Preprocessing text using regex\n",
    "# text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "# text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "# text = re.sub(r'\\s+',' ',text)\n",
    "# text = text.lower()\n",
    "# text = re.sub(r'\\d',' ',text)\n",
    "# text = re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Tokenization\n",
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "words = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "# Stop Words\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words_list = set(stop_words)\n",
    "words_without_stop_words = [i for i in words if i not in stop_words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fisher known fish canal', 'someth fishi go fish dove lake']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    words_without_stop_words = [word for word in review if word not in stop_words_list]\n",
    "    words_stemmed = [stemmer.stem(word) for word in words_without_stop_words ]\n",
    "    words_stemmed = ' '.join(words_stemmed)\n",
    "    corpus.append(words_stemmed)\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fisher known fishing canal', 'something fishy going fish dove lake']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    words_without_stop_words = [word for word in review if word not in stop_words_list]\n",
    "    words_lemmatized = [lemmatizer.lemmatize(word) for word in words_without_stop_words]\n",
    "    words_lemmatized = ' '.join(words_lemmatized)\n",
    "    corpus.append(words_lemmatized)\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 1 0 0 1 0 0]\n",
      " [0 1 1 0 0 1 1 0 1 1]] ['canal', 'dove', 'fish', 'fisher', 'fishing', 'fishy', 'going', 'known', 'lake', 'something']\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words Model\n",
    "# Requires input as list of string sentences\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "x_bow = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "features_bow = cv.get_feature_names()\n",
    "params_bow = cv.get_params()\n",
    "print(x_bow, features_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.         0.         0.5        0.5        0.\n",
      "  0.         0.5        0.         0.        ]\n",
      " [0.         0.40824829 0.40824829 0.         0.         0.40824829\n",
      "  0.40824829 0.         0.40824829 0.40824829]] ['canal', 'dove', 'fish', 'fisher', 'fishing', 'fishy', 'going', 'known', 'lake', 'something']\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Model\n",
    "# Requires input as list of string sentences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfv = TfidfVectorizer()\n",
    "x_tfidf = tfidfv.fit_transform(corpus).toarray()\n",
    "\n",
    "features_tfidf = tfidfv.get_feature_names()\n",
    "params_tfidf = tfidfv.get_params()\n",
    "print(x_tfidf, features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Model\n",
    "# Requires list of string words of each sentence\n",
    "\n",
    "# Lemmatizing\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "corpus_word2vec = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    words_without_stop_words = [word for word in review if word not in stop_words_list]\n",
    "    words_lemmatized = [lemmatizer.lemmatize(word) for word in words_without_stop_words]\n",
    "    corpus_word2vec.append(words_lemmatized)\n",
    "\n",
    "# Word2Vec Model\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(corpus_word2vec, min_count=0)\n",
    "x_vocabulary = model.wv.vocab\n",
    "\n",
    "# Finding Word Vectors\n",
    "vector = model.wv['fishy']\n",
    "# Most similar words\n",
    "similar = model.wv.most_similar('fishy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier using Machine Learning\n",
    "* Import DataFrame, feature engineering\n",
    "* Make a corpus: regex, lower, split, remove stop words,''.join --> corpus\n",
    "* Test Train Split\n",
    "* ML Model building: Multinomial Naive Bayes model\n",
    "* Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "df_spam = pd.read_csv('SMSSpamCollection.txt', sep='\\t', names=['label', 'message'])\n",
    "\n",
    "# Stop Words\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words_list = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "corpus_spam = []\n",
    "\n",
    "for i in range(len(df_spam)):\n",
    "    review = df_spam['message'][i]\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()    \n",
    "    words_without_stop_words = [word for word in review if word not in stop_words_list]\n",
    "    words_lemmatized = [lemmatizer.lemmatize(word) for word in words_without_stop_words]\n",
    "    words_lemmatized = ' '.join(words_lemmatized)\n",
    "    corpus_spam.append(words_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv_spam = CountVectorizer(max_features=2500)\n",
    "x_spam = cv_spam.fit_transform(corpus_spam).toarray()\n",
    "\n",
    "# One hot encode y value and drop a column\n",
    "y_spam = df_spam.drop('message', axis='columns')\n",
    "y_spam = pd.get_dummies(y_spam['label'])\n",
    "y_spam = y_spam['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829596412556054"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_spam, y_spam, test_size = 0.20)\n",
    "\n",
    "# Training model using Naive bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(x_train, y_train)\n",
    "y_predict = spam_detect_model.predict(x_test)\n",
    "spam_detect_model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (Recurrent Neural Network)\n",
    "* A neural network where the output from the previous step is fed as input to the current step\n",
    "* eg: Zuber likes NLP. word1 = Zuber, word2 = likes, word3 = NLP. \n",
    "* ONLY 1 RNN LAYER IS USED. \n",
    "* x<> is a vector i.e 0 or 1. y(hat) is a predicted output at that instance of the RNN. Every instance also outputs a loss\n",
    "* Instance 1: x<0> + word1 --> RNN --> x<1> + y(hat) + Loss1\n",
    "* Instance 2: x<1> + word2 --> RNN --> x<2> + y(hat) + Loss2\n",
    "* Instance 3: x<2> + word3 --> RNN --> x<3> + y(hat) + Loss3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### vanishing gradients and exploding gradients\n",
    "* w(new) = w(old) - (learning rate)x(gradient)\n",
    "* gradient = [ d(Loss)/dw ] = d1xd2xd3... (use chain rule across the hidden layers of the NN)\n",
    "* Gradients are between 0 and 1:\n",
    "    * Hidden layers increases ==> gradient value dimnishes ==> learning rate decreses ==> Backpropagation hardly changes any weights and biases\n",
    "* Gradients are greater than 1:\n",
    "    * Hidden layers increases ==> gradient value increases drastically ==> learning rate increases drastically ==> Backpropagation changes drastically any weights and biases\n",
    "* Solutions: GRU (Gated Recurrent Unit) and LSTM (Long Short Term Memory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSTM\n",
    "* Memory cell (happening at same instance):\n",
    "    * Long term memory: c(t-1) ------------------> c(t)\n",
    "    * Short term memory: x(t-1) + word --> RNN --> x(t)\n",
    "    * RNN = [ weighted sum of x(t-1) + word ] + [ Passed to an tanh or sigmoid activation function ]\n",
    "    * Consists of Forget gate, Input Gate, Output Gate\n",
    "    * More accurate on longer sequence, less efficient\n",
    "\n",
    "###### GRU\n",
    "* Memory cell (happening at same instance):\n",
    "    * Long and Short term memory: x(t-1) + word --> RNN --> x(t)\n",
    "    * Consists of Update gate, Reset Gate\n",
    "    * More efficient, latest method\n",
    "\n",
    "###### Bidirectional RNN\n",
    "* Helps in getting better context of a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing using Tensorflow\n",
    "* Tokenization, One Hot Encoding\n",
    "* Embedding using pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'the': 2, 'fishers': 3, 'are': 4, 'not': 5, 'very': 6, 'known': 7, 'for': 8, 'fishing': 9, 'in': 10, 'canal': 11, 'something': 12, 'fishy': 13, 'was': 14, 'going': 15, 'on': 16, 'some': 17, 'of': 18, 'fish': 19, 'dove': 20, 'into': 21, 'lake': 22}\n",
      "\n",
      "Sequences =  [[3, 4, 5, 6, 7, 8, 9, 10, 2, 11, 12, 13, 14, 15, 16, 17, 18, 2, 19, 20, 21, 2, 22]]\n",
      "\n",
      "Padded Sequences: [[19 20 21  2 22]]\n",
      "\n",
      "Test Sequence =  [[1, 1, 1, 1, 19], [1, 3, 1, 1, 1]]\n",
      "\n",
      "Padded Test Sequence:  [[ 0  0  0  0  0  1  1  1  1 19]\n",
      " [ 0  0  0  0  0  1  3  1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "text = [\"\"\"Fishers are not very known for fishing in the #canal. \n",
    "Something fishy was going on, some of the fish dove into the lake\"\"\"]\n",
    "\n",
    "# Training a tokenizer on given text data\n",
    "# Tokenization: Assigns each word an index\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(text)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "\n",
    "# Sequence of tokens forms the orginal text\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "print(\"\\nSequences = \" , sequences)\n",
    "\n",
    "# Pad Sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "print(\"\\nPadded Sequences:\", padded)\n",
    "\n",
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = ['i really like my fish','my fishers likes my manatee']\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \", padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier using Deep Learning\n",
    "\n",
    "* Import DataFrame, feature engineering\n",
    "* Make a corpus: regex, lower, split, remove stop words,''.join --> corpus\n",
    "* One Hot Encoding or Tokenization\n",
    "* Test Train Split\n",
    "* DL Model building, word embedding done using Embedding layer\n",
    "* Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam_dl = pd.read_csv('SMSSpamCollection.txt', sep='\\t', names=['label', 'message'])\n",
    "\n",
    "# Stop Words\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words_list = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "corpus_spam_dl = []\n",
    "\n",
    "for i in range(len(df_spam)):\n",
    "    review = df_spam['message'][i]\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()    \n",
    "    words_without_stop_words = [word for word in review if word not in stop_words_list]\n",
    "    words_lemmatized = [lemmatizer.lemmatize(word) for word in words_without_stop_words]\n",
    "    words_lemmatized = ' '.join(words_lemmatized)\n",
    "    corpus_spam_dl.append(words_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 6657 8622 2518]\n",
      " [   0    0    0 ... 3968 4687 6449]\n",
      " [6358 9361 3982 ...  408 5203 5158]\n",
      " ...\n",
      " [   0    0    0 ... 3480 9301  401]\n",
      " [   0    0    0 ... 9844 4687 3868]\n",
      " [   0    0    0 ... 7813 9525 6846]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoding\n",
    "vocabulary_size=10000\n",
    "one_hot_encoder = [one_hot(i,vocabulary_size) for i in corpus_spam_dl] \n",
    "# print(one_hot_encoder,'\\n')\n",
    "\n",
    "# Pad Sequences: 'pre' or 'post'\n",
    "sentence_length = 20\n",
    "embedded_text = pad_sequences(one_hot_encoder,padding='pre',maxlen=sentence_length)\n",
    "print(embedded_text, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode y value and drop a column\n",
    "y_spam_dl = df_spam.drop('message', axis='columns')\n",
    "y_spam_dl = pd.get_dummies(y_spam_dl['label'])\n",
    "y_spam_dl = y_spam_dl['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 20, 40)            400000    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 200)               112800    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 513,001\n",
      "Trainable params: 513,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DL Model Building\n",
    "\n",
    "embedding_vector_features=40\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sentence_length))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5572, 20), (5572,))"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_final=np.array(embedded_text)\n",
    "y_final=np.array(y_spam_dl)\n",
    "\n",
    "x_final.shape,y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 3s 44ms/step - loss: 0.0359 - accuracy: 0.9908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17319571c70>"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_dl, x_test_dl, y_train_dl, y_test_dl = train_test_split(x_final, y_final, test_size=0.2, random_state=0)\n",
    "model.fit(x_train,y_train,epochs=1,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[950   5]\n",
      " [ 12 148]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       955\n",
      "           1       0.97      0.93      0.95       160\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.96      0.97      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict_spam_dl = model.predict(x_test_dl)\n",
    "y_predict_spam_labels_dl = [0 if i<0.5 else 1 for i in y_predict_spam_dl]\n",
    "print(confusion_matrix(y_test_dl,y_predict_spam_labels_dl))\n",
    "print(classification_report(y_test_dl,y_predict_spam_labels_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "* Text prediction: ask for input string, generate less words, ask for input string, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f076d1cbe96a44f18846c433d88d18c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YouTube Title: AI learns to love you.\n",
      "\n",
      "Description: The AI has learned to hate you, but it doesn't know what to do with you anymore. It wants to know how you feel about yourself, about your life, and about you\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TFGPT2Model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "sentence = 'YouTube Title: AI learns to'\n",
    "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9aedc409b854f40aad678c9f95dc1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1649.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a085a8a4a748b19476e2ce0b27e553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1222317369.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256a393025cb4c55805b71cee22ad5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898822.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c2c54204684c078adf0e75d0d4e2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61461a08bf334f3392a7c83b176de9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=26.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your min_length is set to 30, but you input_length is only 6. You might consider decreasing min_length manually, e.g. summarizer('...', min_length=10)\n",
      "Your max_length is set to 130, but you input_length is only 6. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': '  Insert text: Insert text . Insert text.  Insert the image . Insert the text:  Insert a photo of a scene from the scene. Insert the photo of the scene from a scene .'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "article = \"\"\" Insert text \"\"\"\n",
    "summarizer(article, max_length=130, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Use Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use transformers pipeline\n",
    "# from transformers import pipeline\n",
    "\n",
    "# # Sentiment Analysis\n",
    "# nlp = pipeline('sentiment-analysis')\n",
    "# nlp('We are very happy to include pipeline into the transformers repository.')\n",
    "\n",
    "\n",
    "# # Question Answering\n",
    "# nlp = pipeline('question-answering')\n",
    "# nlp({\n",
    "#     'question': 'What is my name ?',\n",
    "#     'context': 'I work at HuggingFace'\n",
    "# })\n",
    "\n",
    "# # Predicting Masks\n",
    "# nlp = pipeline('fill-mask')\n",
    "# nlp('I hope you <mask> this video')\n",
    "\n",
    "\n",
    "# # Name Entity Recognition\n",
    "# nlp = pipeline('ner')\n",
    "# nlp('It is me, I work at HuggingFace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
